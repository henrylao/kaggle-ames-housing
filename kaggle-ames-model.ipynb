{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KfApCzuIv2yY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as snb\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ONLZ2n4XwKIT"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')\n",
    "train.SalePrice.skew()\n",
    "\n",
    "# normalize target feature\n",
    "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TcorapxqwXIi"
   },
   "outputs": [],
   "source": [
    "data = pd.concat((train, test)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU5ETfg3weKx"
   },
   "source": [
    "# Handle missing values in features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MUdRwJS3YRHW"
   },
   "outputs": [],
   "source": [
    "# Handle missing values in features\n",
    "\n",
    "data['MiscFeature'] = data['MiscFeature'].fillna('None')\n",
    "data['Alley'] = data['Alley'].fillna('None')\n",
    "data['Fence'] = data['Fence'].fillna('None')\n",
    "data['LotFrontage'].fillna(value=data['LotFrontage'].median(), inplace=True)\n",
    "data.SaleType.fillna(value='Other', inplace=True)\n",
    "\n",
    "data['PoolQC'] = data['PoolQC'].fillna('None')\n",
    "fill_na = {\"None\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4}\n",
    "data['PoolQC'].replace(fill_na, inplace=True)\n",
    "\n",
    "data['FireplaceQu'] = data['FireplaceQu'].fillna('None')\n",
    "fill_na = {\"None\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "data['FireplaceQu'].replace(fill_na, inplace=True)\n",
    "\n",
    "garage_values= data[['GarageYrBlt', 'GarageCond', 'GarageFinish', 'GarageQual', 'GarageType', 'GarageCars', 'GarageArea']][data['GarageType'].isnull()]\n",
    "for i in ['GarageCond', 'GarageFinish', 'GarageQual', 'GarageType']:\n",
    "    data[i].fillna(value='None', inplace=True)\n",
    "    \n",
    "for i in ['GarageYrBlt', 'GarageCars', 'GarageArea']:\n",
    "    data[i].fillna(value=0, inplace=True)\n",
    "\n",
    "fill_na = {\"None\": 0, \"Po\": 1,\"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "data['GarageQual'].replace(fill_na, inplace=True)\n",
    "data['GarageCond'].replace(fill_na, inplace=True)\n",
    "fill_na = {\"None\": 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3}\n",
    "data['GarageFinish'].replace(fill_na, inplace=True)\n",
    "\n",
    "basement_data = data[['BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF', 'TotalBsmtSF']][data['BsmtQual'].isnull()]\n",
    "for i in ['BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual']:\n",
    "    data[i].fillna(value='None', inplace=True)\n",
    "    \n",
    "for i in ['BsmtFinSF1', 'BsmtFinSF2','BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF', 'TotalBsmtSF']:\n",
    "    data[i].fillna(value=0, inplace=True)\n",
    "\n",
    "fill_na = {\"None\": 0, \"Po\": 1,\"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "data['BsmtCond'].replace(fill_na, inplace=True)\n",
    "data['BsmtQual'].replace(fill_na, inplace=True)\n",
    "fill_na = {\"None\": 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4}\n",
    "data['BsmtExposure'].replace(fill_na, inplace=True)\n",
    "fill_na = {\"None\": 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\n",
    "data['BsmtFinType1'].replace(fill_na, inplace=True)\n",
    "data['BsmtFinType2'].replace(fill_na, inplace=True)\n",
    "\n",
    "data['Street'].replace({'Grvl': 0, 'Pave': 1}, inplace=True)\n",
    "\n",
    "data['CentralAir'].replace({'Y': 1, 'N': 0}, inplace=True)\n",
    "\n",
    "fill_na = {\"Po\": 0,\"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4}\n",
    "data.HeatingQC.replace(fill_na, inplace=True)\n",
    "data.ExterQual.replace(fill_na, inplace=True)\n",
    "data.ExterCond.replace(fill_na, inplace=True)\n",
    "\n",
    "fill_na = { 'Sev': 0, 'Mod': 1, 'Gtl': 2 }\n",
    "data.LandSlope.replace(fill_na, inplace=True)\n",
    "\n",
    "fill_na = { 'IR3': 0, 'IR2': 1, 'IR1': 2, 'Reg': 3 }\n",
    "data.LotShape.replace(fill_na, inplace=True)\n",
    "\n",
    "\n",
    "fill_na = {20:'Class1', 30:'Class2', 40:'Class3', 45:'Class4', 50:'Class5', 60:'Class6', 70:'Class7', 75:'Class8', 80:'Class9', 85:'Class10', 90:'Class11', 120:'Class12',150:'Class13', 160:'Class14', 180:'Class15', 190:'Class16'}\n",
    "data.MSSubClass.replace(fill_na, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# impute values\n",
    "mode = data.Electrical.value_counts().idxmax()    \n",
    "data['Electrical'].fillna(mode, inplace=True)\n",
    "\n",
    "mode = data.MasVnrType.value_counts().idxmax() \n",
    "median = data.MasVnrArea.median()     \n",
    "\n",
    "data['MasVnrType'].fillna(mode, inplace=True) \n",
    "data['MasVnrArea'].fillna(median, inplace=True)\n",
    "\n",
    "mode = data.MSZoning.value_counts().idxmax()    \n",
    "data['MSZoning'].fillna(mode, inplace=True)\n",
    "data['MSZoning'].replace({'C (all)': 'C'}, inplace=True)\n",
    "\n",
    "data.drop('Utilities', inplace=True, axis=1)\n",
    "\n",
    "mode = data.Functional.value_counts().idxmax()    \n",
    "data['Functional'].fillna(mode, inplace=True)\n",
    "\n",
    "data.Exterior1st.fillna(value='Other', inplace=True)\n",
    "data.Exterior2nd.fillna(value='Other', inplace=True)\n",
    "\n",
    "mode = data.KitchenQual.value_counts().idxmax()    # 'TA'\n",
    "data['KitchenQual'].fillna(value=mode, inplace=True)\n",
    "fill_na = {\"Po\": 0,\"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4}\n",
    "data['KitchenQual'].replace(fill_na, inplace=True)\n",
    "\n",
    "data_id = data['Id']\n",
    "data.drop('Id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5_79A92XrV4"
   },
   "source": [
    "# Create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "a5_79A92XrV4"
   },
   "outputs": [],
   "source": [
    "data['TotalSF'] = data['1stFlrSF'] + data['2ndFlrSF'] + data['GrLivArea'] + data['TotalBsmtSF']\n",
    "data['Age'] = data.YrSold - data.YearBuilt\n",
    "data['AgeRemod'] = data.YrSold - data.YearRemodAdd\n",
    "data['GarageYrBlt'].replace({0.0: np.nan}, inplace=True)\n",
    "data['GarageYrBlt'].fillna(data['YrSold'], inplace=True)\n",
    "data['AgeGarage'] = data.YrSold - data.GarageYrBlt\n",
    "\n",
    "\n",
    "data.Age       =  data.Age.map(lambda x: 0 if x < 0 else x)\n",
    "data.AgeRemod  =  data.AgeRemod.map(lambda x: 0 if x < 0 else x)\n",
    "data.AgeGarage =  data.AgeGarage.map(lambda x: 0 if x < 0 else x)\n",
    "\n",
    "data.drop(['YrSold','YearBuilt','YearRemodAdd','GarageYrBlt', 'MoSold','FireplaceQu'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "col_drop = data['LotFrontage'][data['LotFrontage'] > 300].index\n",
    "col_drop = np.append(col_drop, data['LotArea'][data['LotArea'] > 100000].index)\n",
    "col_drop = np.append(col_drop, data['BsmtFinSF1'][data['BsmtFinSF1'] > 5000].index)\n",
    "col_drop = np.append(col_drop, data['1stFlrSF'][data['1stFlrSF'] > 4000].index)\n",
    "col_drop = np.append(col_drop, data['GrLivArea'][\n",
    "    (data['GrLivArea'] > 4000) & (data['SalePrice'] < 12.5)\n",
    "].index)\n",
    "train_shape = train.shape[0]\n",
    "test_shape = test.shape[0]\n",
    "col_drop = col_drop[col_drop < train_shape]\n",
    "\n",
    "data = data.drop(col_drop).reset_index(drop=True)\n",
    "\n",
    "dropped_count = len(col_drop)\n",
    "train_shape -= dropped_count\n",
    "\n",
    "col_num = data.skew().index\n",
    "col_skew = data[col_num].skew()[np.abs(data[col_num].skew()) > 0.5].index\n",
    "data[col_skew] = np.log1p(data[col_skew])\n",
    "\n",
    "col_cat = data.select_dtypes(\"object\").columns\n",
    "drop = []\n",
    "for i in col_cat:\n",
    "    drop += [ i+'_'+str(data[i].unique()[-1]) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5_79A92XrV4"
   },
   "source": [
    "# Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "a5_79A92XrV4"
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=col_cat)\n",
    "data.drop(drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "g4HTYx3zYKYx"
   },
   "outputs": [],
   "source": [
    "X_train =  data[:-test_shape].drop(['SalePrice'], axis=1)\n",
    "y_train =  data[:-test_shape]['SalePrice']\n",
    "X_test  =  data[-test_shape:].drop(['SalePrice'], axis=1)\n",
    "col_num = col_num.drop('SalePrice')\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_train[col_num] = scaler.fit_transform(X_train[col_num])\n",
    "X_test[col_num]  = scaler.transform(X_test[col_num])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Models\n",
    "\n",
    "There are 2 parts to the regression:\n",
    "1. Stacked Average of base algorithms and a meta algorithm\n",
    "2. Blending of the Stacked Average with a gradient boosting algorithm (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IZx0surFYgJT"
   },
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "\n",
    "\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "\n",
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "        \n",
    "   \n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "\n",
    "\n",
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n",
    "                                                 meta_model = lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "T6ivHasEmeyV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:22:46] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_xgb.fit(X_train, y_train)\n",
    "xgb_train_pred = model_xgb.predict(X_train)\n",
    "xgb_pred = np.expm1(model_xgb.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BipkrahxmkRR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.2319, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.2319\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=11, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=11\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n"
     ]
    }
   ],
   "source": [
    "model_lgb.fit(X_train, y_train)\n",
    "lgb_train_pred = model_lgb.predict(X_train)\n",
    "lgb_pred = np.expm1(model_lgb.predict(X_test.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8glsDZh_ZMyP"
   },
   "outputs": [],
   "source": [
    "stacked_averaged_models.fit(X_train.values, y_train)\n",
    "stacked_train_pred = stacked_averaged_models.predict(X_train.values)\n",
    "stacked_pred = np.expm1(stacked_averaged_models.predict(X_test.values))\n",
    "ensemble = stacked_pred*0.65 + lgb_pred*0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qxYEDuKxZSGM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Id      SalePrice\n",
      "0  1461  120184.691195\n",
      "1  1462  160988.237588\n",
      "2  1463  185090.241421\n",
      "3  1464  193820.913924\n",
      "4  1465  191734.337869\n"
     ]
    }
   ],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['Id'] = test['Id']\n",
    "sub['SalePrice'] = ensemble\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "print(sub.head())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Ml v4.0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
